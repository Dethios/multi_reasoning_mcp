{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bc9dd7",
   "metadata": {},
   "source": [
    "Below is a full, end‑to‑end implementation you can copy into a repo and run.\n",
    "\n",
    "What you’ll get:\n",
    "\n",
    "* A **custom MCP server** (Python, STDIO) you can plug into **Codex CLI** and/or **Gemini CLI** as a tool server.\n",
    "* Internally, it orchestrates:\n",
    "\n",
    "  * **Codex CLI as an MCP server** (spawned via `npx … codex mcp-server`)\n",
    "  * **Gemini CLI in headless mode** (`gemini -p …`)\n",
    "* Automatic **reasoning-depth selection** per task, mapped to:\n",
    "\n",
    "  * Codex config override: `model_reasoning_effort`\n",
    "  * GPT‑5.2 Agents: `ModelSettings(reasoning.effort=…, verbosity=…)`\n",
    "* “GPT‑5.2 prompting guidance” baked into agent prompts: verbosity clamping, scope discipline, tool rules, ambiguity handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc4de5",
   "metadata": {},
   "source": [
    "## 1) Architecture\n",
    "\n",
    "### Processes\n",
    "\n",
    "1. **Your MCP host** (Codex CLI, Gemini CLI, Cursor, Claude Desktop, etc.)\n",
    "2. **This custom MCP server**: `codex_multireason_mcp` (FastMCP server, STDIO)\n",
    "3. **Inside the server**, on-demand:\n",
    "\n",
    "   * Launches **Codex CLI MCP server**: exposes `codex` and `codex-reply` tools\n",
    "   * Calls **Gemini CLI** headlessly for “second opinion” review\n",
    "\n",
    "### What “multiple reasoning levels” means here\n",
    "\n",
    "* For **Codex** sessions, we set `model_reasoning_effort` per task (fast vs deep) using the MCP `codex` tool’s `config` override (it overrides `$CODEX_HOME/config.toml`).\n",
    "* For **planner/reviewer agents**, we set GPT‑5.2 `reasoning.effort` and `verbosity`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3c3cd",
   "metadata": {},
   "source": [
    "## 2) Prerequisites\n",
    "\n",
    "### System requirements\n",
    "\n",
    "* **Python 3.10+**\n",
    "* **Node.js 18+** (needed for `npx`)\n",
    "* OpenAI API key in environment or `.env`\n",
    "* Gemini CLI installed + API key env var `GEMINI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20466b40",
   "metadata": {},
   "source": [
    "## 3) Create the project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir codex-multireason-mcp\n",
    "cd codex-multireason-mcp\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db83c2",
   "metadata": {},
   "source": [
    "\n",
    "### Install Python deps\n",
    "\n",
    "Agents SDK guide recommends:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9624402",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade openai openai-agents python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee406809",
   "metadata": {},
   "source": [
    "Add MCP server framework:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26311c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install fastmcp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce83c90",
   "metadata": {},
   "source": [
    "\n",
    "FastMCP is a standard way to build MCP servers and supports `FastMCP(...)` + `mcp.run(transport=\"stdio\")`.\n",
    "\n",
    "Optional (recommended):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install pydantic rich\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1602383",
   "metadata": {},
   "source": [
    "## 4) Install / configure Codex CLI\n",
    "\n",
    "You don’t have to global-install Codex if `npx` can fetch it, but you must have a working Codex CLI setup.\n",
    "\n",
    "### Codex config file\n",
    "\n",
    "Codex uses `~/.codex/config.toml`\n",
    "\n",
    "Create/edit:\n",
    "\n",
    "```toml\n",
    "# ~/.codex/config.toml\n",
    "# Base defaults (can be overridden per task by this system via MCP `codex` tool's `config` field)\n",
    "model = \"gpt-5-codex\"\n",
    "model_reasoning_effort = \"medium\"\n",
    "sandbox_mode = \"workspace-write\"\n",
    "approval_policy = \"never\"\n",
    "\n",
    "[profiles.fast]\n",
    "model_reasoning_effort = \"minimal\"\n",
    "\n",
    "[profiles.standard]\n",
    "model_reasoning_effort = \"medium\"\n",
    "\n",
    "[profiles.deep]\n",
    "model_reasoning_effort = \"high\"\n",
    "```\n",
    "\n",
    "Codex supports `model_reasoning_effort` and profiles under `[profiles.<name>]`, and describes precedence (CLI flags > profile > root config > defaults).\n",
    "\n",
    "> Note: exact supported values for `model_reasoning_effort` can vary by Codex version/model. This system will attempt your chosen value and still works even if you only use `low/medium/high`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb294ab0",
   "metadata": {},
   "source": [
    "## 5) Install / configure Gemini CLI\n",
    "\n",
    "### Install\n",
    "\n",
    "```bash\n",
    "npm install -g @google/gemini-cli\n",
    "```\n",
    "\n",
    "### Set API key\n",
    "\n",
    "```bash\n",
    "export GEMINI_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "Gemini CLI codelab uses `GEMINI_API_KEY`.\n",
    "\n",
    "### Config file location\n",
    "\n",
    "* User settings: `~/.gemini/settings.json`\n",
    "* Project settings: `.gemini/settings.json` (in repo)\n",
    "\n",
    "You can optionally define model aliases (helpful for “fast vs deep” reviews). Gemini CLI supports custom aliases under `modelConfigs.customAliases`.\n",
    "\n",
    "Example `~/.gemini/settings.json` snippet (optional):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"modelConfigs\": {\n",
    "    \"customAliases\": {\n",
    "      \"review-fast\": {\n",
    "        \"modelConfig\": { \"model\": \"gemini-2.5-flash\" }\n",
    "      },\n",
    "      \"review-deep\": {\n",
    "        \"modelConfig\": { \"model\": \"gemini-2.5-pro\" }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Headless mode flags\n",
    "\n",
    "Gemini CLI supports non-interactive prompting (`-p`)  and output formatting like `--output-format json` .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16de8c",
   "metadata": {},
   "source": [
    "## 6) Add the implementation files\n",
    "\n",
    "Create this folder structure:\n",
    "\n",
    "```bash\n",
    "mkdir -p codex_multireason_mcp\n",
    "touch codex_multireason_mcp/__init__.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f1412",
   "metadata": {},
   "source": [
    "### 6.1 `codex_multireason_mcp/policy.py`\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "CodexEffort = Literal[\"minimal\", \"low\", \"medium\", \"high\", \"xhigh\"]\n",
    "OpenAIEffort = Literal[\"minimal\", \"low\", \"medium\", \"high\", \"xhigh\"]\n",
    "Verbosity = Literal[\"low\", \"medium\", \"high\"]\n",
    "Sandbox = Literal[\"read-only\", \"workspace-write\"]\n",
    "TaskType = Literal[\"code\", \"review\", \"research\", \"doc\", \"other\"]\n",
    "\n",
    "\n",
    "_LEVEL_ORDER: list[str] = [\"minimal\", \"low\", \"medium\", \"high\", \"xhigh\"]\n",
    "_LEVEL_RANK = {lvl: i for i, lvl in enumerate(_LEVEL_ORDER)}\n",
    "\n",
    "\n",
    "def _max_level(a: str, b: str) -> str:\n",
    "    return a if _LEVEL_RANK[a] >= _LEVEL_RANK[b] else b\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RouteDecision:\n",
    "    task_type: TaskType\n",
    "    codex_effort: CodexEffort\n",
    "    openai_effort: OpenAIEffort\n",
    "    verbosity: Verbosity\n",
    "    sandbox: Sandbox\n",
    "    use_gemini: bool\n",
    "    rationale: str\n",
    "\n",
    "\n",
    "_RISK_KEYWORDS = {\n",
    "    # high-risk / irreversible / security\n",
    "    r\"\\bsecurity\\b\": 4,\n",
    "    r\"\\bauth\\b|\\boauth\\b|\\bjwt\\b\": 3,\n",
    "    r\"\\bcredential\\b|\\bsecret\\b|\\btoken\\b|\\bapi key\\b\": 4,\n",
    "    r\"\\bencrypt\\b|\\bcrypto\\b\": 3,\n",
    "    r\"\\bmigration\\b|\\bdatabase\\b|\\bschema\\b\": 4,\n",
    "    r\"\\bdelete\\b|\\bdrop\\b|\\btruncate\\b\": 4,\n",
    "    r\"\\bproduction\\b|\\bprod\\b\": 3,\n",
    "    r\"\\bcompliance\\b|\\bpolicy\\b|\\bPII\\b|\\bHIPAA\\b|\\bSOX\\b\": 4,\n",
    "}\n",
    "\n",
    "_COMPLEXITY_KEYWORDS = {\n",
    "    r\"\\brefactor\\b|\\bre-architect\\b|\\barchitecture\\b\": 4,\n",
    "    r\"\\bmonorepo\\b|\\bmicroservice\\b\": 3,\n",
    "    r\"\\bperformance\\b|\\boptimi[sz]e\\b\": 2,\n",
    "    r\"\\btest\\b|\\bunit test\\b|\\bintegration\\b\": 2,\n",
    "    r\"\\bCI\\b|\\bCD\\b|\\bpipeline\\b\": 3,\n",
    "    r\"\\bcontainer\\b|\\bdocker\\b|\\bkubernetes\\b\": 3,\n",
    "    r\"\\btypescript\\b|\\brust\\b|\\bgo\\b|\\bpython\\b\": 1,\n",
    "    r\"\\bmultiple files\\b|\\bmulti-file\\b\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "def recommend(task: str, prefer: str = \"auto\") -> RouteDecision:\n",
    "    \"\"\"\n",
    "    Deterministic router. This is the core of \"agents automatically choose best reasoning level\".\n",
    "    prefer: auto|fast|deep\n",
    "    \"\"\"\n",
    "    t = task.strip()\n",
    "    t_lower = t.lower()\n",
    "\n",
    "    # Task type heuristics\n",
    "    if any(k in t_lower for k in [\"diff\", \"review\", \"code review\", \"pr review\"]):\n",
    "        task_type: TaskType = \"review\"\n",
    "    elif any(k in t_lower for k in [\"summarize\", \"rewrite\", \"doc\", \"markdown\", \"readme\"]):\n",
    "        task_type = \"doc\"\n",
    "    elif any(k in t_lower for k in [\"research\", \"compare\", \"options\", \"evaluate\", \"trade-off\"]):\n",
    "        task_type = \"research\"\n",
    "    elif any(k in t_lower for k in [\"bug\", \"fix\", \"implement\", \"refactor\", \"feature\", \"tests\", \"build\"]):\n",
    "        task_type = \"code\"\n",
    "    else:\n",
    "        task_type = \"other\"\n",
    "\n",
    "    risk = 0\n",
    "    for pat, w in _RISK_KEYWORDS.items():\n",
    "        if re.search(pat, t_lower):\n",
    "            risk += w\n",
    "\n",
    "    complexity = 0\n",
    "    for pat, w in _COMPLEXITY_KEYWORDS.items():\n",
    "        if re.search(pat, t_lower):\n",
    "            complexity += w\n",
    "\n",
    "    # Length adds mild complexity\n",
    "    if len(t) > 800:\n",
    "        complexity += 2\n",
    "    elif len(t) > 250:\n",
    "        complexity += 1\n",
    "\n",
    "    # Map scores to levels\n",
    "    if risk >= 10:\n",
    "        base = \"xhigh\"\n",
    "    elif risk >= 6 or complexity >= 8:\n",
    "        base = \"high\"\n",
    "    elif risk >= 3 or complexity >= 5:\n",
    "        base = \"medium\"\n",
    "    elif complexity >= 2:\n",
    "        base = \"low\"\n",
    "    else:\n",
    "        base = \"minimal\"\n",
    "\n",
    "    # User preference override\n",
    "    if prefer == \"fast\":\n",
    "        base = \"minimal\"\n",
    "    elif prefer == \"deep\":\n",
    "        base = \"high\"\n",
    "\n",
    "    # Sandboxing policy\n",
    "    sandbox: Sandbox = \"workspace-write\" if task_type in (\"code\",) else \"read-only\"\n",
    "\n",
    "    # Gemini usage: use for medium+ risk/complexity reviews and any explicit \"compare/evaluate\"\n",
    "    use_gemini = (task_type in (\"review\", \"research\")) or (base in (\"medium\", \"high\", \"xhigh\"))\n",
    "\n",
    "    # Planner effort: usually one notch lower than Codex for speed\n",
    "    openai_effort: OpenAIEffort = base\n",
    "    if base == \"xhigh\":\n",
    "        openai_effort = \"high\"\n",
    "\n",
    "    verbosity: Verbosity = \"low\" if base in (\"minimal\", \"low\") else \"medium\"\n",
    "\n",
    "    rationale = f\"type={task_type}, risk={risk}, complexity={complexity}, prefer={prefer}, chosen={base}\"\n",
    "\n",
    "    return RouteDecision(\n",
    "        task_type=task_type,\n",
    "        codex_effort=base,          # passed to Codex via MCP `config.model_reasoning_effort`\n",
    "        openai_effort=openai_effort,\n",
    "        verbosity=verbosity,\n",
    "        sandbox=sandbox,\n",
    "        use_gemini=use_gemini,\n",
    "        rationale=rationale,\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b37692",
   "metadata": {},
   "source": [
    "### 6.2 `codex_multireason_mcp/gemini_cli.py`\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "class GeminiCLIError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_gemini_headless(\n",
    "    prompt: str,\n",
    "    model: str = \"review-fast\",\n",
    "    timeout_s: int = 120,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls Gemini CLI in headless mode.\n",
    "    - Gemini supports non-interactive prompting via -p. \n",
    "    - Gemini CLI supports --output-format json. \n",
    "    \"\"\"\n",
    "    env = os.environ.copy()\n",
    "\n",
    "    cmd = [\n",
    "        \"gemini\",\n",
    "        \"--model\",\n",
    "        model,\n",
    "        \"--prompt\",\n",
    "        prompt,\n",
    "        \"--output-format\",\n",
    "        \"json\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        p = subprocess.run(\n",
    "            cmd,\n",
    "            input=\"\",\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env=env,\n",
    "            timeout=timeout_s,\n",
    "            check=False,\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        raise GeminiCLIError(\n",
    "            \"gemini CLI not found on PATH. Install it (npm install -g @google/gemini-cli).\"\n",
    "        ) from e\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        raise GeminiCLIError(f\"gemini CLI timed out after {timeout_s}s\") from e\n",
    "\n",
    "    if p.returncode != 0:\n",
    "        raise GeminiCLIError(\n",
    "            f\"gemini CLI failed (exit={p.returncode}). stderr:\\n{p.stderr.strip()}\"\n",
    "        )\n",
    "\n",
    "    raw = p.stdout.strip()\n",
    "\n",
    "    # Best-effort JSON parse (format can vary by version)\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "        return {\"raw_json\": data, \"text\": _extract_text(data) or raw}\n",
    "    except Exception:\n",
    "        return {\"raw_json\": None, \"text\": raw}\n",
    "\n",
    "\n",
    "def _extract_text(obj: Any) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try common shapes. Keep permissive since CLI schemas evolve.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for k in (\"text\", \"output_text\", \"message\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v\n",
    "        # candidates[0].content.parts[0].text style\n",
    "        cands = obj.get(\"candidates\")\n",
    "        if isinstance(cands, list) and cands:\n",
    "            c0 = cands[0]\n",
    "            if isinstance(c0, dict):\n",
    "                content = c0.get(\"content\")\n",
    "                if isinstance(content, dict):\n",
    "                    parts = content.get(\"parts\")\n",
    "                    if isinstance(parts, list) and parts:\n",
    "                        p0 = parts[0]\n",
    "                        if isinstance(p0, dict) and isinstance(p0.get(\"text\"), str):\n",
    "                            return p0[\"text\"]\n",
    "    return None\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2331323",
   "metadata": {},
   "source": [
    "### 6.3 `codex_multireason_mcp/workflow.py`\n",
    "\n",
    "This is where GPT‑5.2 prompting guidance + Codex MCP calls happen.\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "from dataclasses import asdict\n",
    "from typing import Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, ModelSettings, Runner, set_default_openai_api\n",
    "from agents.mcp import MCPServerStdio\n",
    "from openai.types.shared import Reasoning\n",
    "\n",
    "from .policy import RouteDecision\n",
    "from .gemini_cli import run_gemini_headless\n",
    "\n",
    "\n",
    "# GPT‑5.2 prompting guidance emphasizes explicit verbosity clamps, scope discipline,\n",
    "# and crisp tool descriptions. \n",
    "OUTPUT_VERBOSITY_SPEC = \"\"\"\n",
    "<output_verbosity_spec>\n",
    "- Default: 3–6 sentences or ≤5 bullets.\n",
    "- For multi-step tasks: 1 short overview paragraph, then ≤5 bullets tagged:\n",
    "  What changed, Where, Risks, Next steps, Open questions.\n",
    "- Avoid long narrative paragraphs; prefer compact bullets and short sections.\n",
    "- Do not rephrase the user’s request unless it changes semantics.\n",
    "</output_verbosity_spec>\n",
    "\"\"\".strip()\n",
    "\n",
    "SCOPE_DISCIPLINE = \"\"\"\n",
    "<design_and_scope_constraints>\n",
    "- Implement EXACTLY and ONLY what the user requests.\n",
    "- No extra features, no embellishments, no new dependencies unless asked.\n",
    "- If something is ambiguous, choose the simplest valid interpretation.\n",
    "</design_and_scope_constraints>\n",
    "\"\"\".strip()\n",
    "\n",
    "TOOL_RULES = \"\"\"\n",
    "<tool_usage_rules>\n",
    "- Use tools when you need repo-specific truth (file contents, diffs, tests).\n",
    "- Parallelize independent reads when possible.\n",
    "- After any write/update, restate: What changed, Where, and validation performed.\n",
    "</tool_usage_rules>\n",
    "\"\"\".strip()\n",
    "\n",
    "UNCERTAINTY = \"\"\"\n",
    "<uncertainty_and_ambiguity>\n",
    "- If ambiguous, present 2 plausible interpretations with labeled assumptions.\n",
    "- Never invent exact details (paths, outputs) you didn't verify.\n",
    "</uncertainty_and_ambiguity>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _git_available(workspace: str) -> bool:\n",
    "    try:\n",
    "        p = subprocess.run(\n",
    "            [\"git\", \"rev-parse\", \"--is-inside-work-tree\"],\n",
    "            cwd=workspace,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        return p.returncode == 0 and p.stdout.strip() == \"true\"\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _git_diff_bundle(workspace: str, max_chars: int = 120_000) -> dict[str, Any]:\n",
    "    if not _git_available(workspace):\n",
    "        return {\"is_git_repo\": False, \"stat\": None, \"name_only\": None, \"diff\": None}\n",
    "\n",
    "    stat = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--stat\"],\n",
    "        cwd=workspace,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    ).stdout\n",
    "\n",
    "    name_only = subprocess.run(\n",
    "        [\"git\", \"diff\", \"--name-only\"],\n",
    "        cwd=workspace,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    ).stdout\n",
    "\n",
    "    diff = subprocess.run(\n",
    "        [\"git\", \"diff\"],\n",
    "        cwd=workspace,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    ).stdout\n",
    "\n",
    "    if len(diff) > max_chars:\n",
    "        diff = diff[:max_chars] + \"\\n\\n…(diff truncated)…\\n\"\n",
    "\n",
    "    return {\n",
    "        \"is_git_repo\": True,\n",
    "        \"stat\": stat.strip() or None,\n",
    "        \"name_only\": name_only.strip() or None,\n",
    "        \"diff\": diff.strip() or None,\n",
    "    }\n",
    "\n",
    "\n",
    "async def run_workflow(task: str, workspace: str, decision: RouteDecision) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Orchestrates:\n",
    "    1) GPT‑5.2 planner (short plan + acceptance criteria)\n",
    "    2) GPT‑5.2 implementer agent that calls Codex MCP with chosen reasoning effort\n",
    "    3) Optional Gemini CLI review\n",
    "    4) GPT‑5.2 summarizer\n",
    "    \"\"\"\n",
    "    load_dotenv(override=True)\n",
    "    set_default_openai_api(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    planner = Agent(\n",
    "        name=\"Planner\",\n",
    "        model=\"gpt-5.2\",\n",
    "        model_settings=ModelSettings(\n",
    "            reasoning=Reasoning(effort=decision.openai_effort),\n",
    "            verbosity=decision.verbosity,\n",
    "        ),\n",
    "        instructions=\"\\n\\n\".join([\n",
    "            \"You turn user requests into an execution plan with crisp constraints.\",\n",
    "            OUTPUT_VERBOSITY_SPEC,\n",
    "            SCOPE_DISCIPLINE,\n",
    "            TOOL_RULES,\n",
    "            UNCERTAINTY,\n",
    "            \"Output format:\\n\"\n",
    "            \"- 1 short overview paragraph\\n\"\n",
    "            \"- then bullets:\\n\"\n",
    "            \"  - Goals\\n\"\n",
    "            \"  - Non-goals\\n\"\n",
    "            \"  - Acceptance criteria\\n\"\n",
    "            \"  - Risks\\n\"\n",
    "            \"  - Minimal validation steps\\n\",\n",
    "        ]),\n",
    "    )\n",
    "\n",
    "    # Start Codex CLI as an MCP server, as shown in the Agents SDK guide. \n",
    "    async with MCPServerStdio(\n",
    "        name=\"Codex CLI\",\n",
    "        params={\"command\": \"npx\", \"args\": [\"-y\", \"codex\", \"mcp-server\"]},\n",
    "        client_session_timeout_seconds=360000,\n",
    "    ) as codex_mcp_server:\n",
    "\n",
    "        implementer = Agent(\n",
    "            name=\"Implementer\",\n",
    "            model=\"gpt-5.2\",\n",
    "            model_settings=ModelSettings(\n",
    "                reasoning=Reasoning(effort=decision.openai_effort),\n",
    "                verbosity=\"low\",\n",
    "            ),\n",
    "            # Attaches Codex MCP tools to this agent. \n",
    "            mcp_servers=[codex_mcp_server],\n",
    "            instructions=\"\\n\\n\".join([\n",
    "                \"You implement the plan by calling the Codex MCP tool.\",\n",
    "                \"Rules:\",\n",
    "                \"- You MUST call the `codex` tool at least once for implementation work.\",\n",
    "                \"- Use sandbox strictly as instructed.\",\n",
    "                \"- Do not add extra features or dependencies.\",\n",
    "                OUTPUT_VERBOSITY_SPEC,\n",
    "                SCOPE_DISCIPLINE,\n",
    "                TOOL_RULES,\n",
    "                UNCERTAINTY,\n",
    "                \"\",\n",
    "                \"When calling Codex MCP, ALWAYS use JSON with these fields (Codex MCP supports them): \"\n",
    "                \"`prompt`, `approval-policy`, `sandbox`, and optional `config` overrides. \",\n",
    "                \"\",\n",
    "                \"Use this exact parameter shape:\",\n",
    "                f\"\"\"\n",
    "{{\n",
    "  \"prompt\": \"...\",\n",
    "  \"approval-policy\": \"never\",\n",
    "  \"sandbox\": \"{decision.sandbox}\",\n",
    "  \"config\": {{\n",
    "    \"model_reasoning_effort\": \"{decision.codex_effort}\"\n",
    "  }},\n",
    "  \"cwd\": \"{workspace}\"\n",
    "}}\n",
    "\"\"\".strip(),\n",
    "                \"\",\n",
    "                \"After Codex completes, respond with ONLY:\",\n",
    "                \"- 1 line: 'Codex run complete.'\",\n",
    "            ]),\n",
    "        )\n",
    "\n",
    "        summarizer = Agent(\n",
    "            name=\"Summarizer\",\n",
    "            model=\"gpt-5.2\",\n",
    "            model_settings=ModelSettings(\n",
    "                reasoning=Reasoning(effort=\"low\"),\n",
    "                verbosity=\"medium\",\n",
    "            ),\n",
    "            instructions=\"\\n\\n\".join([\n",
    "                \"You produce the final user-facing summary.\",\n",
    "                OUTPUT_VERBOSITY_SPEC,\n",
    "                \"If you see potential risk or missing validation, call it out in 'Risks' and 'Next steps'.\",\n",
    "            ]),\n",
    "        )\n",
    "\n",
    "        plan_res = await Runner.run(planner, task)\n",
    "        plan_text = plan_res.final_output\n",
    "\n",
    "        impl_input = (\n",
    "            \"Task:\\n\"\n",
    "            f\"{task}\\n\\n\"\n",
    "            \"Plan:\\n\"\n",
    "            f\"{plan_text}\\n\\n\"\n",
    "            \"Now implement using Codex MCP tool.\\n\"\n",
    "        )\n",
    "        _ = await Runner.run(implementer, impl_input)\n",
    "\n",
    "    # After Codex finishes, collect diff (outside Codex, deterministic)\n",
    "    diff_bundle = _git_diff_bundle(workspace)\n",
    "\n",
    "    gemini_review = None\n",
    "    if decision.use_gemini and diff_bundle.get(\"diff\"):\n",
    "        gem_prompt = (\n",
    "            \"You are a meticulous code reviewer.\\n\"\n",
    "            \"Review this diff for correctness, edge cases, security, and missing tests.\\n\"\n",
    "            \"Return:\\n\"\n",
    "            \"- Findings (bullets)\\n\"\n",
    "            \"- Severity per finding (low/med/high)\\n\"\n",
    "            \"- Suggested fixes (bullets)\\n\\n\"\n",
    "            f\"Task:\\n{task}\\n\\n\"\n",
    "            f\"Diff stat:\\n{diff_bundle.get('stat')}\\n\\n\"\n",
    "            f\"Diff:\\n{diff_bundle.get('diff')}\\n\"\n",
    "        )\n",
    "        # Choose model alias depending on effort (optional)\n",
    "        model_alias = \"review-deep\" if decision.codex_effort in (\"high\", \"xhigh\") else \"review-fast\"\n",
    "        gemini_review = run_gemini_headless(gem_prompt, model=model_alias)\n",
    "\n",
    "    summary_input = (\n",
    "        \"Summarize the completed work.\\n\\n\"\n",
    "        f\"Routing decision: {decision.rationale}\\n\\n\"\n",
    "        f\"Plan:\\n{plan_text}\\n\\n\"\n",
    "        f\"Git diff stat:\\n{diff_bundle.get('stat')}\\n\\n\"\n",
    "        f\"Changed files:\\n{diff_bundle.get('name_only')}\\n\\n\"\n",
    "        f\"Gemini review:\\n{(gemini_review or {}).get('text')}\\n\"\n",
    "    )\n",
    "\n",
    "    summary_res = await Runner.run(summarizer, summary_input)\n",
    "\n",
    "    return {\n",
    "        \"decision\": asdict(decision),\n",
    "        \"plan\": plan_text,\n",
    "        \"git\": diff_bundle,\n",
    "        \"gemini_review\": gemini_review,\n",
    "        \"final\": summary_res.final_output,\n",
    "    }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d6542",
   "metadata": {},
   "source": [
    "Key points this uses from docs:\n",
    "\n",
    "* Codex MCP `codex` tool supports `config` overrides and `sandbox` modes\n",
    "* Codex supports `model_reasoning_effort` in config.toml and via CLI `--config model_reasoning_effort=\"high\"`\n",
    "* Agents SDK runs Codex MCP via `MCPServerStdio(... npx -y codex mcp-server ...)`\n",
    "* GPT‑5.2 prompting guidance: verbosity clamps, scope discipline, tool rules\n",
    "* Agents SDK supports `ModelSettings(reasoning.effort=…, verbosity=…)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d946d5a",
   "metadata": {},
   "source": [
    "### 6.4 `codex_multireason_mcp/server.py` (Your custom MCP server)\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "from .policy import recommend\n",
    "from .workflow import run_workflow\n",
    "from .gemini_cli import run_gemini_headless\n",
    "\n",
    "\n",
    "mcp = FastMCP(\"codex-multireason-mcp\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def recommend_route(task: str, prefer: str = \"auto\") -> dict:\n",
    "    \"\"\"\n",
    "    Tool: returns the chosen reasoning/sandbox policy without executing anything.\n",
    "    \"\"\"\n",
    "    d = recommend(task, prefer=prefer)\n",
    "    return {\n",
    "        \"task\": task,\n",
    "        \"prefer\": prefer,\n",
    "        \"decision\": d.__dict__,\n",
    "    }\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def run(task: str, workspace: str = \".\", prefer: str = \"auto\") -> dict:\n",
    "    \"\"\"\n",
    "    Tool: runs the full workflow (plan -> codex -> optional gemini -> summary).\n",
    "    \"\"\"\n",
    "    d = recommend(task, prefer=prefer)\n",
    "    return await run_workflow(task=task, workspace=workspace, decision=d)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def gemini(prompt: str, model: str = \"review-fast\") -> dict:\n",
    "    \"\"\"\n",
    "    Tool: direct access to Gemini CLI headless, useful for ad-hoc second opinions.\n",
    "    \"\"\"\n",
    "    return run_gemini_headless(prompt, model=model)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # Serve MCP over STDIO (typical for desktop/CLI hosts). \n",
    "    mcp.run(transport=\"stdio\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b9cfd",
   "metadata": {},
   "source": [
    "### 6.5 `codex_multireason_mcp/cli.py` (local smoke test without any MCP host)\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "from .policy import recommend\n",
    "from .workflow import run_workflow\n",
    "\n",
    "\n",
    "async def _main() -> None:\n",
    "    if len(sys.argv) < 2:\n",
    "        print('Usage: python -m codex_multireason_mcp.cli \"your task here\" [workspace] [prefer]')\n",
    "        raise SystemExit(2)\n",
    "\n",
    "    task = sys.argv[1]\n",
    "    workspace = sys.argv[2] if len(sys.argv) >= 3 else \".\"\n",
    "    prefer = sys.argv[3] if len(sys.argv) >= 4 else \"auto\"\n",
    "\n",
    "    decision = recommend(task, prefer=prefer)\n",
    "    result = await run_workflow(task, workspace, decision)\n",
    "    print(result[\"final\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(_main())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606c140",
   "metadata": {},
   "source": [
    "## 7) Run it locally (smoke test)\n",
    "\n",
    "Create `.env`:\n",
    "\n",
    "```bash\n",
    "cat > .env << 'EOF'\n",
    "OPENAI_API_KEY=sk-...\n",
    "# GEMINI_API_KEY=...  # only needed if you want Gemini reviews\n",
    "EOF\n",
    "```\n",
    "\n",
    "Now run:\n",
    "\n",
    "```bash\n",
    "python -m codex_multireason_mcp.cli \"Refactor the config loader to support profiles and add tests.\"\n",
    "```\n",
    "\n",
    "You should see a short, structured final summary, and if this is a git repo you’ll also get a diff bundle in the returned object (when run via MCP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20440914",
   "metadata": {},
   "source": [
    "## 8) Use it as an MCP server\n",
    "\n",
    "### Option A: Connect from Codex CLI\n",
    "\n",
    "Codex MCP server configuration lives in `~/.codex/config.toml`\n",
    "\n",
    "Add:\n",
    "\n",
    "```toml\n",
    "[mcp_servers.codex_multireason]\n",
    "command = \"python\"\n",
    "args = [\"-m\", \"codex_multireason_mcp.server\"]\n",
    "\n",
    "[mcp_servers.codex_multireason.env]\n",
    "OPENAI_API_KEY = \"sk-...\"   # or omit and rely on env\n",
    "GEMINI_API_KEY = \"...\"      # optional\n",
    "```\n",
    "\n",
    "Codex supports MCP server entries via `[mcp_servers.<name>]` with `command`, `args`, and optional env.\n",
    "\n",
    "Then launch Codex and check `/mcp` in the TUI.\n",
    "\n",
    "### Option B: Connect from Gemini CLI as a tool host\n",
    "\n",
    "Gemini CLI supports `mcpServers` in `settings.json`  and configuration files at `~/.gemini/settings.json` or `.gemini/settings.json`.\n",
    "\n",
    "Example `~/.gemini/settings.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"codex_multireason\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"-m\", \"codex_multireason_mcp.server\"],\n",
    "      \"env\": {\n",
    "        \"OPENAI_API_KEY\": \"sk-...\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Now Gemini CLI can call your MCP tools when prompted (depending on its tool-routing behavior).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a374c72",
   "metadata": {},
   "source": [
    "## 9) How the “automatic reasoning” works (and how to tune it)\n",
    "\n",
    "### Current policy behavior (deterministic)\n",
    "\n",
    "* “typo/format/small fix” ⇒ `minimal`\n",
    "* “refactor/tests/perf” ⇒ `medium`\n",
    "* “architecture/migration/security” ⇒ `high` or `xhigh`\n",
    "* Code-changing tasks ⇒ `sandbox=\"workspace-write\"`, otherwise `read-only`\n",
    "\n",
    "This is the knob Codex exposes as “Reasoning depth” via `model_reasoning_effort`.\n",
    "And the Codex MCP tool lets us override config per session via `config`.\n",
    "\n",
    "### GPT‑5.2 prompt shaping used here\n",
    "\n",
    "We embed the guide’s best practices:\n",
    "\n",
    "* Explicit verbosity clamps\n",
    "* Scope drift prevention\n",
    "* Tool usage rules\n",
    "\n",
    "### Quick customization points\n",
    "\n",
    "* Edit `policy.py` keyword weights\n",
    "* Add an explicit “prefer=fast|deep” argument from your MCP host\n",
    "* Modify `workflow.py` to:\n",
    "\n",
    "  * always run Gemini on `high+`\n",
    "  * use “review-deep” alias whenever diff > N lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74fd857",
   "metadata": {},
   "source": [
    "## 10) Notes & operational caveats\n",
    "\n",
    "* **Gemini headless mode** (`-p`) can’t authorize interactive tools or run shell commands. That’s fine here because we only use it for text review.\n",
    "* Codex MCP tool has `approval-policy` values like `untrusted`, `on-failure`, `never` and `sandbox` modes like `read-only` / `workspace-write` / `danger-full-access`.\n",
    "  This implementation uses `approval-policy=\"never\"` to avoid blocking automation, and uses sandboxing as the safety boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cde958",
   "metadata": {},
   "source": [
    "## 11) What to do next (recommended upgrades)\n",
    "\n",
    "If you want this to feel “production grade”:\n",
    "\n",
    "1. **Keep Codex MCP server warm**\n",
    "   Right now, each `run()` call starts a new Codex MCP subprocess. You can optimize by managing a long-lived Codex MCP server instance inside FastMCP (single process, reused per call).\n",
    "\n",
    "2. **Progressive deepening**\n",
    "\n",
    "   * Start with `low`\n",
    "   * If tests fail or diff touches sensitive files, re-run with `high`\n",
    "\n",
    "3. **Add a “guardrail tool”**\n",
    "\n",
    "   * Block risky commands\n",
    "   * Require explicit user override for `danger-full-access`\n",
    "\n",
    "If you tell me your typical task categories (e.g., “mostly refactors + CI”, “mostly docs + small fixes”, “often security reviews”), I can give you a tuned `policy.py` profile and stronger, role-specific prompt blocks (e.g., a dedicated “Security Reviewer” agent that always forces `sandbox=\"read-only\"` and `high` effort).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
